{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1c3f52-e7ac-41da-9f67-a80ab61d1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.linalg import eig\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7fb375-be27-4a69-b0a5-2b2d6ed59f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)  # if using multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6ba1d6-d547-454f-93b6-4e3ac1277d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import linalg\n",
    "from statsmodels.tsa.api import VAR\n",
    "from scipy import stats\n",
    "\n",
    "def compute_spillover_index(data, horizon, lag, scarcity_prop, standardized=True):\n",
    "    # Input data should be np.array\n",
    "    data_array = data.values\n",
    "    # Fit VAR model\n",
    "    model = VAR(data_array)\n",
    "    results = model.fit(maxlags=lag)\n",
    "    \n",
    "    # Manually Compute Forecast Error Variance Decomposition (FEVD)\n",
    "    Sigma = results.sigma_u\n",
    "    A = results.orth_ma_rep(maxn=horizon - 1)\n",
    "    \n",
    "    Sigma_A = []\n",
    "    A_Sigma_A = []\n",
    "    \n",
    "    for h in range(horizon):\n",
    "        # Numerator\n",
    "        Sigma_A_h = (A[h] @ Sigma @ np.linalg.inv(np.diag(np.sqrt(np.diag(Sigma))))) ** 2\n",
    "        Sigma_A.append(Sigma_A_h)\n",
    "        \n",
    "        # Denominator\n",
    "        A_Sigma_A_h = A[h] @ Sigma @ A[h].T\n",
    "        A_Sigma_A.append(A_Sigma_A_h)\n",
    "    \n",
    "    # Numerator: cumulative sum of Sigma_A\n",
    "    num = np.cumsum(Sigma_A, axis=0)\n",
    "    \n",
    "    # Denominator: cumulative sum of A_Sigma_A\n",
    "    den = np.cumsum(A_Sigma_A, axis=0)\n",
    "    \n",
    "    # Generalized FEVD\n",
    "    gfevd = np.array([num[h] / np.diag(den[h])[:, None] for h in range(horizon)])\n",
    "    \n",
    "    if standardized:\n",
    "        # Standardize each FEVD matrix so that each row sums to 1\n",
    "        gfevd = np.array([fevd / fevd.sum(axis=1, keepdims=True) for fevd in gfevd])\n",
    "    \n",
    "    # Aggregate results over n_ahead steps\n",
    "    spillover_matrix = gfevd[-1]\n",
    "    \n",
    "    # VSP from row to column so can be used as adjacency matrix\n",
    "    spillover_matrix = spillover_matrix.T ## row --> column: if node i --> node j, A_{ij} != 0\n",
    "    \n",
    "    # Convert to percentage\n",
    "    spillover_matrix *= 100      \n",
    "    \n",
    "    # Calculate 'to' and 'from' others\n",
    "    K = spillover_matrix.shape[0]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(spillover_matrix, columns=results.names, index=results.names)\n",
    "    \n",
    "    # Increase sparcity\n",
    "    vsp_df_sparse = results_df.copy()\n",
    "    threshold = pd.Series(results_df.values.flatten()).quantile(scarcity_prop)\n",
    "    vsp_df_sparse[vsp_df_sparse < threshold] = 0\n",
    "    vsp_np_sparse = vsp_df_sparse.values\n",
    "    np.fill_diagonal(vsp_np_sparse, 0)\n",
    "\n",
    "    if standardized:\n",
    "        vsp_np_sparse = vsp_np_sparse / K\n",
    "        return vsp_np_sparse\n",
    "    else:\n",
    "        return vsp_np_sparse # for each train_x batch, dim(results_array) = [num_node, num_node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a636f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "def save_model(name, model, num_L = None, best_loss_val = None):\n",
    "    if not os.path.exists('models/'):\n",
    "            os.makedirs('models/')\n",
    "    # Prepare the model state dictionary\n",
    "    config = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'layer': num_L,\n",
    "        'loss': best_loss_val\n",
    "    }\n",
    "    # Save the model state dictionary\n",
    "    torch.save(config, f'models/{name}.tar')\n",
    "    return\n",
    "\n",
    "## Load model\n",
    "def load_model(name, model):\n",
    "    checkpoint = torch.load(f'models/{name}.tar', map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    num_L = checkpoint['layer']\n",
    "    mae_loss = checkpoint['loss']\n",
    "    print(f\"Loaded model: {name}\")\n",
    "    print(f\"MAE loss: {mae_loss}\")\n",
    "    return model, mae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113654c2",
   "metadata": {},
   "source": [
    "### GSP-HAR-dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "627ef0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSPHAR(nn.Module):\n",
    "    # linear transformation\n",
    "    def __init__ (self, input_dim, output_dim, filter_size, A):\n",
    "        super(GSPHAR,self).__init__()\n",
    "        self.A = torch.from_numpy(A)\n",
    "        self.filter_size = filter_size\n",
    "        self.conv1d_lag5 = nn.Conv1d(in_channels = filter_size, out_channels = filter_size, kernel_size = 5, groups = filter_size, bias = False) # groups=1 markets share similarity\n",
    "        nn.init.constant_(self.conv1d_lag5.weight, 1.0 / 5)\n",
    "        self.conv1d_lag22 = nn.Conv1d(in_channels = filter_size, out_channels = filter_size, kernel_size = 22, groups = filter_size, bias = False) # groups=1\n",
    "        nn.init.constant_(self.conv1d_lag22.weight, 1.0 / 22)\n",
    "        self.spatial_process = nn.Sequential(\n",
    "            nn.Linear(2, 2 * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * 8, 2 * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * 8, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear_output = nn.Linear(input_dim, output_dim, bias=True)\n",
    "    \n",
    "    def nomalized_magnet_laplacian(self, A, q, norm = True):\n",
    "        A_s = (A + A.T)/2\n",
    "        D_s = np.diag(np.sum(A_s, axis=1))\n",
    "        pi = np.pi\n",
    "        theta_q = 2 * pi * q * (A - A.T)\n",
    "        H_q = A_s * np.exp(1j*theta_q)\n",
    "        if norm == True:\n",
    "            D_s_inv = np.linalg.inv(D_s)\n",
    "            D_s_inv_sqrt = sqrtm(D_s_inv)\n",
    "            L = np.eye(len(D_s)) - (D_s_inv_sqrt @ A_s @ D_s_inv_sqrt) * np.exp(1j*theta_q)\n",
    "        else:\n",
    "            L = D_s - H_q\n",
    "\n",
    "        return L\n",
    "    \n",
    "    def dynamic_magnet_Laplacian(self, A, x_lag_p, x_lag_q): # p < q\n",
    "        A = (A - A.T)\n",
    "        A[A < 0] = 0\n",
    "        \n",
    "        # Center the data by subtracting the mean along the last dimension (axis=2)\n",
    "        mean_p = x_lag_p.mean(dim=2, keepdim=True)\n",
    "        x_lag_p_centered = x_lag_p - mean_p\n",
    "        # Compute the covariance matrices: (batch_size, num_features, num_features)\n",
    "        cov_matrices_p = torch.bmm(x_lag_p_centered, x_lag_p_centered.transpose(1, 2)) / (x_lag_p.shape[2] - 1)\n",
    "        # Compute the standard deviations: (batch_size, num_features)\n",
    "        stddev_p = x_lag_p_centered.std(dim=2, unbiased=True)\n",
    "        # Avoid division by zero\n",
    "        stddev_p[stddev_p == 0] = 1\n",
    "        # Outer product of the standard deviations to get a matrix of standard deviations for normalization\n",
    "        stddev_matrix_p = stddev_p.unsqueeze(2) * stddev_p.unsqueeze(1)\n",
    "        # Compute the correlation matrices by normalizing the covariance matrices\n",
    "        correlation_matrices_p = cov_matrices_p / stddev_matrix_p\n",
    "        abs_correlation_matrices_p = abs(correlation_matrices_p)\n",
    "\n",
    "        # Center the data by subtracting the mean along the last dimension (axis=2)\n",
    "        mean_q = x_lag_q.mean(dim=2, keepdim=True)\n",
    "        x_lag_q_centered = x_lag_q - mean_q\n",
    "        # Compute the covariance matrices: (batch_size, num_features, num_features)\n",
    "        cov_matrices_q = torch.bmm(x_lag_q_centered, x_lag_q_centered.transpose(1, 2)) / (x_lag_q.shape[2] - 1)\n",
    "        # Compute the standard deviations: (batch_size, num_features)\n",
    "        stddev_q = x_lag_q_centered.std(dim=2, unbiased=True)\n",
    "        # Avoid division by zero\n",
    "        stddev_q[stddev_q == 0] = 1\n",
    "        # Outer product of the standard deviations to get a matrix of standard deviations for normalization\n",
    "        stddev_matrix_q = stddev_q.unsqueeze(2) * stddev_q.unsqueeze(1)\n",
    "        # Compute the correlation matrices by normalizing the covariance matrices\n",
    "        correlation_matrices_q = cov_matrices_q / stddev_matrix_q\n",
    "        abs_correlation_matrices_q = abs(correlation_matrices_q)\n",
    "\n",
    "        alpha = 0.5\n",
    "        \n",
    "        A_p = abs_correlation_matrices_p * A\n",
    "        A_q = abs_correlation_matrices_q * A\n",
    "        \n",
    "        A =  alpha * (A_p) +  (1.-alpha) * A_q\n",
    "\n",
    "        # normalization\n",
    "        A = F.softmax(A, dim=1)\n",
    "        \n",
    "        U_dega_list = []\n",
    "        U_list = []\n",
    "        for i in range(len(A)):\n",
    "            sub_A = A[i]\n",
    "            sub_L = self.nomalized_magnet_laplacian(sub_A.cpu().numpy(), 0.25)\n",
    "            eigenvalues, eigenvectors = eig(sub_L)\n",
    "            sub_Lambda = eigenvalues.real\n",
    "            sub_U_dega = eigenvectors\n",
    "            sub_U = eigenvectors.T.conj()\n",
    "            sorted_indices = np.argsort(sub_Lambda)  # Sort in ascending order\n",
    "            sub_U_dega_sorted = sub_U_dega[sorted_indices, :]\n",
    "            sub_U_sorted = sub_U[:, sorted_indices] \n",
    "            sub_U_dega =  torch.complex(torch.tensor(sub_U_dega.real, dtype=torch.float32), torch.tensor(sub_U_dega.imag, dtype=torch.float32))\n",
    "            sub_U =  torch.complex(torch.tensor(sub_U.real, dtype=torch.float32), torch.tensor(sub_U.imag, dtype=torch.float32))\n",
    "            U_dega_list.append(sub_U_dega)\n",
    "            U_list.append(sub_U)\n",
    "\n",
    "            U_dega = torch.stack(U_dega_list)\n",
    "            U = torch.stack(U_list)\n",
    "            return U_dega, U \n",
    "        \n",
    "    def forward(self, x_lag1, x_lag5, x_lag22):\n",
    "        # Ensure all items are on the same device as the input x\n",
    "        device = x_lag1.device\n",
    "        A = self.A.to(device)\n",
    "        self.conv1d_lag5 = self.conv1d_lag5.to(device)\n",
    "        self.conv1d_lag22 = self.conv1d_lag22.to(device)\n",
    "        self.spatial_process = self.spatial_process.to(device)\n",
    "        self.linear_output_real = self.linear_output.to(device)\n",
    "        self.linear_output_imag = self.linear_output.to(device)\n",
    "        \n",
    "        # Compute dynamic adj_mx\n",
    "        U_dega, U = self.dynamic_magnet_Laplacian(A,x_lag5, x_lag22)\n",
    "        U_dega = U_dega.to(device)\n",
    "        U = U.to(device)\n",
    "    \n",
    "        # Convert RV to complex domain\n",
    "        x_lag1 = torch.complex(x_lag1, torch.zeros_like(x_lag1))\n",
    "        x_lag5 = torch.complex(x_lag5, torch.zeros_like(x_lag5))\n",
    "        x_lag22 = torch.complex(x_lag22, torch.zeros_like(x_lag22))\n",
    "\n",
    "        # Spectral domain operations on lag-5\n",
    "        x_lag5 = torch.matmul(U_dega, x_lag5)\n",
    "        exp_param_5 = torch.exp(self.conv1d_lag5.weight)\n",
    "        sum_exp_param_5 = torch.sum(exp_param_5, dim=-1, keepdim=True)\n",
    "        softmax_param_5 = exp_param_5/sum_exp_param_5\n",
    "        x_lag5_real = F.conv1d(input=x_lag5.real, weight=softmax_param_5, bias=None, groups=self.filter_size)\n",
    "        x_lag5_imag = F.conv1d(input=x_lag5.imag, weight=softmax_param_5, bias=None, groups=self.filter_size)\n",
    "        x_lag5 = torch.complex(x_lag5_real, x_lag5_imag)\n",
    "        x_lag5 = x_lag5.squeeze(-1)\n",
    "\n",
    "        # Spectral domain operations on lag-22\n",
    "        x_lag22 = torch.matmul(U_dega, x_lag22)\n",
    "        exp_param_22 = torch.exp(self.conv1d_lag22.weight)\n",
    "        sum_exp_param_22 = torch.sum(exp_param_22, dim=-1, keepdim=True)\n",
    "        softmax_param_22 = exp_param_22/sum_exp_param_22\n",
    "        x_lag22_real = F.conv1d(input=x_lag22.real, weight=softmax_param_22, bias=None, groups=self.filter_size)\n",
    "        x_lag22_imag = F.conv1d(input=x_lag22.imag, weight=softmax_param_22, bias=None, groups=self.filter_size)\n",
    "        x_lag22 = torch.complex(x_lag22_real, x_lag22_imag)\n",
    "        x_lag22 = x_lag22.squeeze(-1)\n",
    "\n",
    "        # Lag-1 processing\n",
    "        x_lag1 = torch.matmul(U_dega, x_lag1.unsqueeze(-1))\n",
    "        x_lag1 = x_lag1.squeeze(-1)\n",
    "\n",
    "        # Combine lagged responses in the spectral domain\n",
    "        lagged_rv_spectral = torch.stack((x_lag1, x_lag5, x_lag22), dim=-1)\n",
    "        \n",
    "        # Apply linear transformation separately to the real and imaginary parts\n",
    "        y_hat_real = self.linear_output_real(lagged_rv_spectral.real)\n",
    "        y_hat_imag = self.linear_output_imag(lagged_rv_spectral.imag)\n",
    "        y_hat_spectral = torch.complex(y_hat_real, y_hat_imag)\n",
    "\n",
    "        # Back to the spatial domain\n",
    "        y_hat = torch.matmul(U, y_hat_spectral)\n",
    "\n",
    "        y_hat_real = y_hat.real # [batch_size, num_markets, 1]\n",
    "        y_hat_imag = y_hat.imag # [batch_size, num_markets, 1]\n",
    "        y_hat_real = y_hat_real.squeeze(-1)\n",
    "        y_hat_imag = y_hat_imag.squeeze(-1)\n",
    "        \n",
    "        y_hat_spatial = torch.stack((y_hat_real, y_hat_imag), dim=-1)\n",
    "        \n",
    "        # Apply linear transformation separately to the real and imaginary parts\n",
    "        y_hat = self.spatial_process(y_hat_spatial)\n",
    "        \n",
    "        return y_hat.squeeze(-1), softmax_param_5, softmax_param_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae441072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSPHAR_Dataset(Dataset):\n",
    "    def __init__(self, dict):\n",
    "        self.dict = dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dict.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        date = list(self.dict.keys())[idx]\n",
    "        dfs_dict = self.dict[date]\n",
    "        y = dfs_dict['y'].values\n",
    "        x_lag1 = dfs_dict['x_lag1'].values\n",
    "        x_lag5 = dfs_dict['x_lag5'].values\n",
    "        x_lag22 = dfs_dict['x_lag22'].values\n",
    "        \n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        x_lag1_tensor = torch.tensor(x_lag1, dtype=torch.float32)\n",
    "        x_lag5_tensor = torch.tensor(x_lag5, dtype=torch.float32)\n",
    "        x_lag22_tensor = torch.tensor(x_lag22, dtype=torch.float32)\n",
    "        return x_lag1_tensor, x_lag5_tensor, x_lag22_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce33b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_model(model, dataloader_train, dataloader_test, num_epochs = 200, lr = 0.01):\n",
    "    best_loss_val = 1000000\n",
    "    patience = 0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = lr, \n",
    "                                                   steps_per_epoch=len(dataloader_train), epochs = num_epochs,\n",
    "                                                   three_phase=True)\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion = criterion.to(device)\n",
    "    model.train()\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for x_lag1, x_lag5, x_lag22, y in dataloader_train:\n",
    "            x_lag1 = x_lag1.to(device)\n",
    "            x_lag5 = x_lag5.to(device)\n",
    "            x_lag22 = x_lag22.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, conv1d_lag5_weights, conv1d_lag22_weights = model(x_lag1, x_lag5, x_lag22)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update scheduler: this scheduler is designed to be updated after each batch.\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Evaluate model\n",
    "        valid_loss = evaluate_model(model, dataloader_test)\n",
    "\n",
    "        if valid_loss < best_loss_val:\n",
    "            best_loss_val = valid_loss\n",
    "            final_conv1d_lag5_weights = conv1d_lag5_weights.detach().cpu().numpy()\n",
    "            final_conv1d_lag22_weights = conv1d_lag22_weights.detach().cpu().numpy()\n",
    "            patience = 0\n",
    "            save_model(f'GSPHAR_24_magnet_dynamic_h{h}', model, None, best_loss_val)\n",
    "        else:\n",
    "            patience = patience + 1\n",
    "            if patience >= 200:\n",
    "                print(f'early stopping at epoch {epoch+1}.')\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "    return best_loss_val, final_conv1d_lag5_weights, final_conv1d_lag22_weights\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, dataloader_test):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    criterion = criterion.to(device)\n",
    "    valid_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_lag1, x_lag5, x_lag22, y in dataloader_test:\n",
    "            x_lag1 = x_lag1.to(device)\n",
    "            x_lag5 = x_lag5.to(device)\n",
    "            x_lag22 = x_lag22.to(device)\n",
    "            y = y.to(device)\n",
    "            output, _, _ = model(x_lag1, x_lag5, x_lag22)\n",
    "            loss = criterion(output, y)\n",
    "            valid_loss = valid_loss + loss.item()\n",
    "    valid_loss = valid_loss/len(dataloader_test)\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a172a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch 395.\n",
      "Loaded model: GSPHAR_24_magnet_dynamic_h5\n",
      "MAE loss: 0.2244093355257064\n"
     ]
    }
   ],
   "source": [
    "h = 5\n",
    "data = pd.read_csv('rv5_sqrt_24.csv', index_col = 0)*100\n",
    "date_list = data.index.tolist()\n",
    "train_end_idx = int(len(date_list)*0.7)\n",
    "train_dataset = data.iloc[0:train_end_idx,:]\n",
    "test_dataset = data.iloc[train_end_idx:,:]\n",
    "\n",
    "market_indices_list = train_dataset.columns.tolist()\n",
    "\n",
    "DY_adj = compute_spillover_index(train_dataset, h, 22, 0.0, standardized=True)\n",
    "\n",
    "look_back_window = 22\n",
    "for market_index in market_indices_list:\n",
    "    for lag in range(look_back_window):\n",
    "        train_dataset[market_index + f'_{lag+1}'] = train_dataset[market_index].shift(lag+h)\n",
    "        test_dataset[market_index + f'_{lag+1}'] = test_dataset[market_index].shift(lag+h)\n",
    "\n",
    "train_dataset = train_dataset.dropna()\n",
    "test_dataset = test_dataset.dropna()\n",
    "\n",
    "columns_lag1 = [x for x in train_dataset.columns.tolist() if x[-2:] == '_1']\n",
    "columns_lag5 = [x for x in train_dataset.columns.tolist() if (x[-2]=='_') and (float(x[-1]) in range(1,6))]\n",
    "columns_lag22 = [x for x in train_dataset.columns.tolist() if '_' in x]\n",
    "x_columns = columns_lag1 + columns_lag5 + columns_lag22\n",
    "y_columns = [x for x in train_dataset.columns.tolist() if x not in x_columns]\n",
    "row_index_order = market_indices_list\n",
    "column_index_order_5 = [f'lag_{i}' for i in range(1,6)]\n",
    "column_index_order_22 = [f'lag_{i}' for i in range(1,23)]\n",
    "\n",
    "\n",
    "train_dict = {}\n",
    "for date in train_dataset.index:\n",
    "    y = train_dataset.loc[date,y_columns]\n",
    "    \n",
    "    x_lag1 = train_dataset.loc[date,columns_lag1]\n",
    "    new_index = [ind[:-2] for ind in x_lag1.index.tolist()]\n",
    "    x_lag1.index = new_index\n",
    "    \n",
    "    x_lag5 = train_dataset.loc[date,columns_lag5]\n",
    "\n",
    "    # Split the index into market indices and lags\n",
    "    data_lag5 = {\n",
    "    'Market': [index.split('_')[0] for index in x_lag5.index],\n",
    "    'Lag': [f'lag_{index.split(\"_\")[1]}' for index in x_lag5.index],\n",
    "    'Value': x_lag5.values\n",
    "    }\n",
    "    # Convert to DataFrame\n",
    "    df_lag5 = pd.DataFrame(data_lag5)\n",
    "    # Pivot the DataFrame\n",
    "    df_lag5 = df_lag5.pivot(index='Market', columns='Lag', values='Value')\n",
    "\n",
    "    x_lag22 = train_dataset.loc[date,columns_lag22]\n",
    "    # Split the index into market indices and lags\n",
    "    data_lag22 = {\n",
    "    'Market': [index.split('_')[0] for index in x_lag22.index],\n",
    "    'Lag': [f'lag_{index.split(\"_\")[1]}' for index in x_lag22.index],\n",
    "    'Value': x_lag22.values\n",
    "    }\n",
    "    # Convert to DataFrame\n",
    "    df_lag22 = pd.DataFrame(data_lag22)\n",
    "    # Pivot the DataFrame\n",
    "    df_lag22 = df_lag22.pivot(index='Market', columns='Lag', values='Value')\n",
    "\n",
    "    x_lag1 = x_lag1.reindex(row_index_order)\n",
    "    df_lag5 = df_lag5.reindex(row_index_order)\n",
    "    df_lag22= df_lag22.reindex(row_index_order)\n",
    "    df_lag5 = df_lag5[column_index_order_5]\n",
    "    df_lag22 = df_lag22[column_index_order_22]\n",
    "    \n",
    "    dfs_dict = {\n",
    "        'y': y,\n",
    "        'x_lag1': x_lag1,\n",
    "        'x_lag5': df_lag5,\n",
    "        'x_lag22': df_lag22\n",
    "    }\n",
    "    train_dict[date] = dfs_dict\n",
    "\n",
    "test_dict = {}\n",
    "for date in test_dataset.index:\n",
    "    y = test_dataset.loc[date,y_columns]\n",
    "    \n",
    "    x_lag1 = test_dataset.loc[date,columns_lag1]\n",
    "    new_index = [ind[:-2] for ind in x_lag1.index.tolist()]\n",
    "    x_lag1.index = new_index\n",
    "    \n",
    "    x_lag5 = test_dataset.loc[date,columns_lag5]\n",
    "    # Split the index into market indices and lags\n",
    "    data_lag5 = {\n",
    "    'Market': [index.split('_')[0] for index in x_lag5.index],\n",
    "    'Lag': [f'lag_{index.split(\"_\")[1]}' for index in x_lag5.index],\n",
    "    'Value': x_lag5.values\n",
    "    }\n",
    "    # Convert to DataFrame\n",
    "    df_lag5 = pd.DataFrame(data_lag5)\n",
    "    # Pivot the DataFrame\n",
    "    df_lag5 = df_lag5.pivot(index='Market', columns='Lag', values='Value')\n",
    "\n",
    "    x_lag22 = test_dataset.loc[date,columns_lag22]\n",
    "    # Split the index into market indices and lags\n",
    "    data_lag22 = {\n",
    "    'Market': [index.split('_')[0] for index in x_lag22.index],\n",
    "    'Lag': [f'lag_{index.split(\"_\")[1]}' for index in x_lag22.index],\n",
    "    'Value': x_lag22.values\n",
    "    }\n",
    "    # Convert to DataFrame\n",
    "    df_lag22 = pd.DataFrame(data_lag22)\n",
    "    # Pivot the DataFrame\n",
    "    df_lag22 = df_lag22.pivot(index='Market', columns='Lag', values='Value')\n",
    "\n",
    "    x_lag1 = x_lag1.reindex(row_index_order)\n",
    "    df_lag5 = df_lag5.reindex(row_index_order)\n",
    "    df_lag22= df_lag22.reindex(row_index_order)\n",
    "    df_lag5 = df_lag5[column_index_order_5]\n",
    "    df_lag22 = df_lag22[column_index_order_22]\n",
    "    \n",
    "    dfs_dict = {\n",
    "        'y': y,\n",
    "        'x_lag1': x_lag1,\n",
    "        'x_lag5': df_lag5,\n",
    "        'x_lag22': df_lag22\n",
    "    }\n",
    "    test_dict[date] = dfs_dict\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset_train = GSPHAR_Dataset(train_dict)\n",
    "dataset_test = GSPHAR_Dataset(test_dict)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 1\n",
    "filter_size = 24\n",
    "num_epochs = 500\n",
    "lr = 0.01\n",
    "\n",
    "GSPHAR_RV = GSPHAR(input_dim,output_dim, filter_size, DY_adj)\n",
    "valid_loss, final_conv1d_lag5_weights, final_conv1d_lag22_weights = train_eval_model(GSPHAR_RV, dataloader_train, dataloader_test, num_epochs, lr)\n",
    "trained_GSPHAR, mae_GSPHAR  = load_model(f'GSPHAR_24_magnet_dynamic_h{h}',GSPHAR_RV) \n",
    "\n",
    "y_hat_list = []\n",
    "y_list = []\n",
    "\n",
    "trained_GSPHAR.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_lag1, x_lag5, x_lag22, y in dataloader_test:\n",
    "        y_hat, _, _ = trained_GSPHAR(x_lag1, x_lag5, x_lag22)\n",
    "        \n",
    "       # Append the predicted and actual values to their respective lists\n",
    "        y_hat_list.append(y_hat.cpu().numpy())\n",
    "        y_list.append(y.cpu().numpy())\n",
    "\n",
    "y_hat_concatenated = np.concatenate(y_hat_list, axis=0)\n",
    "y_concatenated = np.concatenate(y_list, axis=0)\n",
    "\n",
    "rv_hat_GSPHAR_dynamic = pd.DataFrame(data = y_hat_concatenated, columns = market_indices_list)\n",
    "rv_true = pd.DataFrame(data = y_concatenated, columns = market_indices_list)\n",
    "\n",
    "pred_GSPHAR_dynamic_df = pd.DataFrame()\n",
    "for market_index in market_indices_list:\n",
    "    pred_column = market_index+'_rv_forecast'\n",
    "    true_column = market_index+'_rv_true'\n",
    "    pred_GSPHAR_dynamic_df[pred_column] = rv_hat_GSPHAR_dynamic[market_index]\n",
    "    pred_GSPHAR_dynamic_df[true_column] = rv_true[market_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3f2ee-0366-48ff-894b-297701a79a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e099f-9fe1-448c-b67f-f940329515e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
