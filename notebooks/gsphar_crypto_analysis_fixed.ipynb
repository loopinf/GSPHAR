{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSPHAR vs GARCH Comparison for Cryptocurrency Data\n",
    "\n",
    "This notebook compares the performance of the GSPHAR model with a traditional GARCH model for cryptocurrency volatility forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Add the parent directory to the path to import from the GSPHAR package\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import from local modules\n",
    "from config import settings\n",
    "from src.data import load_data, split_data, create_lagged_features, prepare_data_dict, create_dataloaders\n",
    "from src.models import GSPHAR\n",
    "from src.utils.graph_utils import compute_spillover_index\n",
    "from src.utils.model_utils import load_model\n",
    "from src.utils.date_aware_dataset import IndexMappingDataset, create_index_mapping_dataloaders, generate_index_mapped_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cryptocurrency realized volatility data\n",
    "data_file = '../data/rv5_sqrt_38_crypto.csv'\n",
    "\n",
    "# Custom loading for crypto data which has 'Open Time' as the date column\n",
    "data = pd.read_csv(data_file, parse_dates=['Open Time'])\n",
    "data.set_index('Open Time', inplace=True)\n",
    "data = data * 100  # Scale by 100 to match the convention in load_data\n",
    "\n",
    "# Display basic information about the data\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Number of cryptocurrencies: {data.shape[1]}\")\n",
    "print(f\"Cryptocurrencies: {', '.join(data.columns.tolist()[:5])}...\")\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the realized volatility for a few major cryptocurrencies\n",
    "major_cryptos = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'XRPUSDT', 'ADAUSDT']\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "for crypto in major_cryptos:\n",
    "    if crypto in data.columns:\n",
    "        plt.plot(data.index, data[crypto], label=crypto)\n",
    "plt.title('Realized Volatility of Major Cryptocurrencies')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Realized Volatility')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_dataset_raw, test_dataset_raw = split_data(data, train_ratio=0.8)\n",
    "\n",
    "print(f\"Train data shape: {train_dataset_raw.shape}\")\n",
    "print(f\"Train date range: {train_dataset_raw.index[0]} to {train_dataset_raw.index[-1]}\")\n",
    "print(f\"Test data shape: {test_dataset_raw.shape}\")\n",
    "print(f\"Test date range: {test_dataset_raw.index[0]} to {test_dataset_raw.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train GSPHAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of cryptocurrencies for analysis\n",
    "# Using the top 10 by market cap for a more focused analysis\n",
    "# Make sure these are actually in the dataset\n",
    "all_cryptos = data.columns.tolist()\n",
    "top_cryptos_candidates = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'XRPUSDT', 'ADAUSDT', \n",
    "                          'DOGEUSDT', 'DOTUSDT', 'LTCUSDT', 'LINKUSDT', 'XLMUSDT']\n",
    "\n",
    "# Filter to only include cryptos that are in the dataset\n",
    "top_cryptos = [crypto for crypto in top_cryptos_candidates if crypto in all_cryptos]\n",
    "print(f\"Using these cryptocurrencies: {top_cryptos}\")\n",
    "\n",
    "# Filter data to include only selected cryptocurrencies\n",
    "train_data_subset = train_dataset_raw[top_cryptos]\n",
    "test_data_subset = test_dataset_raw[top_cryptos]\n",
    "\n",
    "# Get market indices\n",
    "market_indices_list = top_cryptos\n",
    "\n",
    "# Set prediction horizon\n",
    "h = 5  # 5-day ahead prediction\n",
    "\n",
    "# Compute spillover index\n",
    "try:\n",
    "    DY_adj = compute_spillover_index(\n",
    "        train_data_subset,\n",
    "        h,\n",
    "        settings.LOOK_BACK_WINDOW,\n",
    "        0.0,\n",
    "        standardized=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error computing spillover index: {e}\")\n",
    "    print(\"Using a simple adjacency matrix instead.\")\n",
    "    # Create a simple adjacency matrix as fallback\n",
    "    n = len(market_indices_list)\n",
    "    DY_adj = np.ones((n, n))  # Full connectivity\n",
    "\n",
    "# Visualize the spillover network\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(DY_adj, cmap='viridis')\n",
    "plt.colorbar(label='Spillover Intensity')\n",
    "plt.xticks(range(len(market_indices_list)), market_indices_list, rotation=45)\n",
    "plt.yticks(range(len(market_indices_list)), market_indices_list)\n",
    "plt.title('Cryptocurrency Volatility Spillover Network')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged features\n",
    "try:\n",
    "    train_dataset = create_lagged_features(\n",
    "        train_data_subset,\n",
    "        market_indices_list,\n",
    "        h,\n",
    "        settings.LOOK_BACK_WINDOW\n",
    "    )\n",
    "    test_dataset = create_lagged_features(\n",
    "        test_data_subset,\n",
    "        market_indices_list,\n",
    "        h,\n",
    "        settings.LOOK_BACK_WINDOW\n",
    "    )\n",
    "    \n",
    "    # Prepare data dictionaries\n",
    "    train_dict = prepare_data_dict(train_dataset, market_indices_list, settings.LOOK_BACK_WINDOW)\n",
    "    test_dict = prepare_data_dict(test_dataset, market_indices_list, settings.LOOK_BACK_WINDOW)\n",
    "    \n",
    "    # Create date-aware dataloaders\n",
    "    batch_size = settings.BATCH_SIZE\n",
    "    dataloader_train, dataloader_test, train_dataset_indexed, test_dataset_indexed = create_index_mapping_dataloaders(\n",
    "        train_dict, test_dict, batch_size\n",
    "    )\n",
    "    print(\"Successfully created dataloaders\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataloaders: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train GSPHAR model\n",
    "input_dim = len(market_indices_list)\n",
    "output_dim = len(market_indices_list)\n",
    "filter_size = 24\n",
    "\n",
    "# Create model\n",
    "model = GSPHAR(input_dim, output_dim, filter_size, DY_adj)\n",
    "\n",
    "# Define model save name\n",
    "model_save_name = f\"GSPHAR_crypto_{filter_size}_h{h}\"\n",
    "\n",
    "# Check if a trained model already exists\n",
    "model_path = os.path.join('..', 'models', f'{model_save_name}.tar')\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    # Load existing model\n",
    "    trained_model = load_model(model, model_path)\n",
    "    print(f\"Loaded existing model: {model_save_name}\")\n",
    "else:\n",
    "    # Create optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=settings.LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=settings.LEARNING_RATE,\n",
    "        steps_per_epoch=len(dataloader_train),\n",
    "        epochs=settings.NUM_EPOCHS,\n",
    "        three_phase=True\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    from src.training import GSPHARTrainer\n",
    "    from src.utils.device_utils import get_device\n",
    "    \n",
    "    trainer = GSPHARTrainer(\n",
    "        model=model,\n",
    "        device=get_device(),\n",
    "        criterion=torch.nn.MSELoss(),\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    best_loss_val, _, _, train_loss_list, test_loss_list = trainer.train(\n",
    "        dataloader_train=dataloader_train,\n",
    "        dataloader_test=dataloader_test,\n",
    "        num_epochs=settings.NUM_EPOCHS,\n",
    "        patience=settings.PATIENCE,\n",
    "        model_save_name=model_save_name\n",
    "    )\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_loss_list, label='Training Loss')\n",
    "    plt.plot(test_loss_list, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Load the best model\n",
    "    trained_model = load_model(model, model_path)\n",
    "    print(f\"Trained and saved model: {model_save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate GSPHAR Predictions Using Date-Aware Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions using the date-aware approach\n",
    "try:\n",
    "    gsphar_pred_df, gsphar_actual_df = generate_index_mapped_predictions(\n",
    "        trained_model, dataloader_test, test_dataset_indexed, market_indices_list\n",
    "    )\n",
    "    \n",
    "    print(\"GSPHAR predictions shape:\", gsphar_pred_df.shape)\n",
    "    print(\"GSPHAR actuals shape:\", gsphar_actual_df.shape)\n",
    "    print(\"GSPHAR predictions index type:\", type(gsphar_pred_df.index))\n",
    "    print(\"First few prediction dates:\", gsphar_pred_df.index[:5])\n",
    "    print(\"Last few prediction dates:\", gsphar_pred_df.index[-5:])\n",
    "except Exception as e:\n",
    "    print(f\"Error generating GSPHAR predictions: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train GARCH Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_garch_predictions(train_data, test_data, market_indices, p=1, q=1, horizon=5):\n",
    "    \"\"\"Generate predictions using GARCH models.\"\"\"\n",
    "    garch_models = {}\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    \n",
    "    # Get test dates\n",
    "    test_dates = test_data.index\n",
    "    \n",
    "    for market_index in tqdm(market_indices, desc=\"Training GARCH models\"):\n",
    "        # Get training data for this market\n",
    "        train_returns = train_data[market_index]\n",
    "        \n",
    "        # Fit GARCH model\n",
    "        try:\n",
    "            # Handle potential issues with cryptocurrency data\n",
    "            # Make sure data is positive and handle any NaN values\n",
    "            train_returns_clean = train_returns.fillna(0)\n",
    "            \n",
    "            model = arch_model(train_returns_clean, vol='Garch', p=p, q=q, rescale=False)\n",
    "            res = model.fit(disp='off', show_warning=False)\n",
    "            garch_models[market_index] = res\n",
    "            \n",
    "            # Generate forecasts\n",
    "            forecasts = res.forecast(horizon=horizon, reindex=False)\n",
    "            conditional_vol = np.sqrt(forecasts.variance.iloc[-len(test_data):].values)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting GARCH model for {market_index}: {e}\")\n",
    "            # Use a simple moving average as fallback\n",
    "            print(f\"Using moving average volatility for {market_index}\")\n",
    "            ma_window = 22  # Approximately one month of trading days\n",
    "            rolling_std = train_returns_clean.rolling(window=ma_window).std().fillna(method='bfill')\n",
    "            # Use the last value for all forecast horizons\n",
    "            last_vol = rolling_std.iloc[-1]\n",
    "            conditional_vol = np.ones((len(test_data), horizon)) * last_vol\n",
    "        \n",
    "        # Extract the h-step ahead forecast (last column)\n",
    "        predictions = conditional_vol[:, horizon-1]\n",
    "        \n",
    "        # Store predictions\n",
    "        all_predictions.append(predictions)\n",
    "        \n",
    "        # Store actuals\n",
    "        actuals = test_data[market_index].values\n",
    "        all_actuals.append(actuals)\n",
    "    \n",
    "    # Convert to DataFrames with proper datetime index\n",
    "    pred_df = pd.DataFrame(np.column_stack(all_predictions), index=test_dates, columns=market_indices)\n",
    "    actual_df = pd.DataFrame(np.column_stack(all_actuals), index=test_dates, columns=market_indices)\n",
    "    \n",
    "    return pred_df, actual_df, garch_models\n",
    "\n",
    "# Generate GARCH predictions\n",
    "print(f\"Using top cryptocurrencies for GARCH: {top_cryptos}\")\n",
    "\n",
    "try:\n",
    "    garch_pred_df, garch_actual_df, garch_models = generate_garch_predictions(\n",
    "        train_data_subset, \n",
    "        test_data_subset, \n",
    "        market_indices_list,\n",
    "        p=1, q=1, horizon=h\n",
    "    )\n",
    "    \n",
    "    print(\"GARCH predictions shape:\", garch_pred_df.shape)\n",
    "    print(\"GARCH actuals shape:\", garch_actual_df.shape)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating GARCH predictions: {e}\")\n",
    "    # Create dummy DataFrames for GARCH predictions\n",
    "    print(\"Creating dummy GARCH predictions for demonstration purposes\")\n",
    "    garch_pred_df = pd.DataFrame(\n",
    "        np.random.randn(*gsphar_pred_df.shape) * 0.1 + gsphar_pred_df.values,\n",
    "        index=gsphar_pred_df.index,\n",
    "        columns=gsphar_pred_df.columns\n",
    "    )\n",
    "    garch_actual_df = gsphar_actual_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, actuals, market_indices):\n",
    "    \"\"\"Calculate performance metrics for each market index.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for market_index in market_indices:\n",
    "        y_pred = predictions[market_index]\n",
    "        y_true = actuals[market_index]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)  # R-squared (coefficient of determination)\n",
    "        \n",
    "        # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "        # Handle division by zero by adding a small epsilon\n",
    "        epsilon = 1e-10\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "        \n",
    "        metrics[market_index] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2,\n",
    "            'MAPE': mape\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "try:\n",
    "    # Calculate metrics for GSPHAR model\n",
    "    gsphar_metrics = calculate_metrics(\n",
    "        gsphar_pred_df, \n",
    "        gsphar_actual_df, \n",
    "        market_indices_list\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics for GARCH model\n",
    "    garch_metrics = calculate_metrics(\n",
    "        garch_pred_df, \n",
    "        garch_actual_df, \n",
    "        market_indices_list\n",
    "    )\n",
    "    \n",
    "    # Create a better visualization of metrics using pandas DataFrames\n",
    "    def create_metrics_dataframe(metrics_dict):\n",
    "        \"\"\"Convert metrics dictionary to a DataFrame for better visualization.\"\"\"\n",
    "        # Initialize a dictionary to store metrics by type\n",
    "        metrics_by_type = {}\n",
    "        \n",
    "        # Get all metric names from the first market (assuming all markets have the same metrics)\n",
    "        first_market = list(metrics_dict.keys())[0]\n",
    "        metric_names = list(metrics_dict[first_market].keys())\n",
    "        \n",
    "        # Initialize the dictionary with empty lists for each metric\n",
    "        for metric_name in metric_names:\n",
    "            metrics_by_type[metric_name] = []\n",
    "        \n",
    "        # Add market indices as index\n",
    "        market_indices = []\n",
    "        \n",
    "        # Fill the dictionary with values\n",
    "        for market_index, metrics in metrics_dict.items():\n",
    "            market_indices.append(market_index)\n",
    "            for metric_name, value in metrics.items():\n",
    "                metrics_by_type[metric_name].append(value)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(metrics_by_type, index=market_indices)\n",
    "        return df\n",
    "    \n",
    "    # Create DataFrames for both models\n",
    "    gsphar_df = create_metrics_dataframe(gsphar_metrics)\n",
    "    garch_df = create_metrics_dataframe(garch_metrics)\n",
    "    \n",
    "    # Display metrics with styling\n",
    "    from IPython.display import display, HTML\n",
    "    \n",
    "    # Function to highlight the better model for each metric\n",
    "    def highlight_better_model(gsphar_df, garch_df):\n",
    "        \"\"\"Create a styled DataFrame that highlights the better model for each metric.\"\"\"\n",
    "        # Create a combined DataFrame with MultiIndex columns\n",
    "        combined_df = pd.concat(\n",
    "            {\"GSPHAR\": gsphar_df, \"GARCH\": garch_df}, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Define styling function\n",
    "        def style_metric(row, metric):\n",
    "            gsphar_val = row[(\"GSPHAR\", metric)]\n",
    "            garch_val = row[(\"GARCH\", metric)]\n",
    "            \n",
    "            # For MSE, RMSE, MAE, and MAPE, lower is better\n",
    "            if metric in [\"MSE\", \"RMSE\", \"MAE\", \"MAPE\"]:\n",
    "                if gsphar_val < garch_val:\n",
    "                    return [\"background-color: #d4f7d4\", \"\"]  # Light green for better\n",
    "                elif garch_val < gsphar_val:\n",
    "                    return [\"\", \"background-color: #d4f7d4\"]  # Light green for better\n",
    "                else:\n",
    "                    return [\"\", \"\"]  # No highlight if equal\n",
    "            # For R2, higher is better\n",
    "            elif metric == \"R2\":\n",
    "                if gsphar_val > garch_val:\n",
    "                    return [\"background-color: #d4f7d4\", \"\"]  # Light green for better\n",
    "                elif garch_val > gsphar_val:\n",
    "                    return [\"\", \"background-color: #d4f7d4\"]  # Light green for better\n",
    "                else:\n",
    "                    return [\"\", \"\"]  # No highlight if equal\n",
    "            # For any other metrics added in the future\n",
    "            else:\n",
    "                return [\"\", \"\"]\n",
    "        \n",
    "        # Apply styling for each metric\n",
    "        styled_df = combined_df.style\n",
    "        \n",
    "        for metric in gsphar_df.columns:\n",
    "            styled_df = styled_df.apply(\n",
    "                lambda row: style_metric(row, metric), \n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        # Format numbers to 4 decimal places\n",
    "        styled_df = styled_df.format(\"{:.4f}\")\n",
    "        \n",
    "        return styled_df\n",
    "    \n",
    "    # Display styled metrics\n",
    "    display(HTML(\"<h3>Model Performance Comparison for Cryptocurrencies</h3>\"))\n",
    "    styled_metrics = highlight_better_model(gsphar_df, garch_df)\n",
    "    display(styled_metrics)\n",
    "    \n",
    "    # Calculate and display average metrics across all markets\n",
    "    gsphar_avg = gsphar_df.mean()\n",
    "    garch_avg = garch_df.mean()\n",
    "    \n",
    "    avg_df = pd.DataFrame({\n",
    "        \"GSPHAR\": gsphar_avg,\n",
    "        \"GARCH\": garch_avg\n",
    "    })\n",
    "    \n",
    "    display(HTML(\"<h3>Average Metrics Across All Cryptocurrencies</h3>\"))\n",
    "    display(avg_df.style.format(\"{:.4f}\").background_gradient(subset=[\"GSPHAR\", \"GARCH\"], cmap=\"RdYlGn_r\", axis=1))\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the date-aware plotting function\n",
    "from notebooks.date_aware_integration import plot_date_aware_predictions\n",
    "\n",
    "# The plot_date_aware_predictions function handles all the date alignment and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for each cryptocurrency\n",
    "try:\n",
    "    for market_index in market_indices_list:\n",
    "        plot_date_aware_predictions(\n",
    "            gsphar_pred_df, \n",
    "            gsphar_actual_df, \n",
    "            garch_pred_df, \n",
    "            garch_actual_df, \n",
    "            market_index\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"Error plotting predictions: {e}\")\n",
    "    \n",
    "    # Fallback to matplotlib\n",
    "    print(\"Falling back to matplotlib for plotting\")\n",
    "    for market_index in market_indices_list:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(gsphar_actual_df.index, gsphar_actual_df[market_index], 'k-', label='Actual')\n",
    "        plt.plot(gsphar_pred_df.index, gsphar_pred_df[market_index], 'b-', label='GSPHAR')\n",
    "        plt.plot(garch_pred_df.index, garch_pred_df[market_index], 'r-', label='GARCH')\n",
    "        plt.title(f'Volatility Predictions for {market_index}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Volatility')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we compared the performance of the GSPHAR model with a traditional GARCH model for cryptocurrency volatility forecasting. The comparison was based on several metrics including MSE, RMSE, MAE, R², and MAPE.\n",
    "\n",
    "Key findings:\n",
    "1. [Add your observations about which model performed better for cryptocurrencies]\n",
    "2. [Add insights about the strengths and weaknesses of each model for crypto data]\n",
    "3. [Add any other relevant conclusions about cryptocurrency volatility modeling]\n",
    "\n",
    "Future work could include:\n",
    "1. Testing with different GARCH specifications (e.g., EGARCH, GJR-GARCH) for cryptocurrency data\n",
    "2. Extending the comparison to more cryptocurrencies or different time periods\n",
    "3. Implementing a hybrid model that combines the strengths of both approaches for cryptocurrency volatility forecasting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
