{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSPHAR Model Input Inspection\n",
    "\n",
    "This notebook focuses specifically on examining the input shapes, data transformation, and actual values that are fed into the GSPHAR model. It provides a detailed look at how time series data is processed and structured for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add the parent directory to the path to import from the GSPHAR package\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import from local modules\n",
    "from config import settings\n",
    "from src.data import load_data, split_data, create_lagged_features\n",
    "from src.utils.date_aware_dataset import IndexMappingDataset\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cryptocurrency realized volatility data\n",
    "data_file = '../data/rv5_sqrt_38_crypto.csv'\n",
    "\n",
    "# Custom loading for crypto data which has 'Open Time' as the date column\n",
    "try:\n",
    "    data = pd.read_csv(data_file, parse_dates=['Open Time'])\n",
    "    data.set_index('Open Time', inplace=True)\n",
    "    data = data * 100  # Scale by 100 to match the convention in load_data\n",
    "except Exception as e:\n",
    "    print(f\"Error loading cryptocurrency data with custom loader: {e}\")\n",
    "    print(\"Falling back to standard load_data function\")\n",
    "    data = load_data(data_file)\n",
    "\n",
    "# Display basic information about the data\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"Number of cryptocurrencies: {data.shape[1]}\")\n",
    "print(f\"Cryptocurrencies: {', '.join(data.columns.tolist()[:5])}...\")\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_dataset_raw, test_dataset_raw = split_data(data, train_ratio=0.8)\n",
    "\n",
    "print(f\"Train data shape: {train_dataset_raw.shape}\")\n",
    "print(f\"Train date range: {train_dataset_raw.index[0]} to {train_dataset_raw.index[-1]}\")\n",
    "print(f\"Test data shape: {test_dataset_raw.shape}\")\n",
    "print(f\"Test date range: {test_dataset_raw.index[0]} to {test_dataset_raw.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of cryptocurrencies for analysis\n",
    "# Using the top 10 by market cap for a more focused analysis\n",
    "# Make sure these are actually in the dataset\n",
    "all_cryptos = data.columns.tolist()\n",
    "top_cryptos_candidates = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'XRPUSDT', 'ADAUSDT', \n",
    "                          'DOGEUSDT', 'DOTUSDT', 'LTCUSDT', 'LINKUSDT', 'XLMUSDT']\n",
    "\n",
    "# Filter to only include cryptos that are in the dataset\n",
    "top_cryptos = [crypto for crypto in top_cryptos_candidates if crypto in all_cryptos]\n",
    "print(f\"Using these cryptocurrencies: {top_cryptos}\")\n",
    "\n",
    "# Filter data to include only selected cryptocurrencies\n",
    "train_data_subset = train_dataset_raw[top_cryptos]\n",
    "test_data_subset = test_dataset_raw[top_cryptos]\n",
    "\n",
    "# Get market indices\n",
    "market_indices_list = top_cryptos\n",
    "\n",
    "# Set prediction horizon\n",
    "h = 5  # 5-day ahead prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Lagged Features\n",
    "\n",
    "Now we'll create the lagged features that will be used as input to the model. This is a critical step in the data transformation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged features\n",
    "try:\n",
    "    train_dataset = create_lagged_features(\n",
    "        train_data_subset,\n",
    "        market_indices_list,\n",
    "        h,\n",
    "        settings.LOOK_BACK_WINDOW\n",
    "    )\n",
    "    test_dataset = create_lagged_features(\n",
    "        test_data_subset,\n",
    "        market_indices_list,\n",
    "        h,\n",
    "        settings.LOOK_BACK_WINDOW\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset with lagged features shape: {train_dataset.shape}\")\n",
    "    print(f\"Test dataset with lagged features shape: {test_dataset.shape}\")\n",
    "    \n",
    "    # Display the column names to see the lagged features\n",
    "    print(\"\\nFirst 20 column names:\")\n",
    "    print(train_dataset.columns[:20])\n",
    "    \n",
    "    # Display the first few rows of the dataset with lagged features\n",
    "    print(\"\\nFirst few rows of the dataset with lagged features:\")\n",
    "    display(train_dataset.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error creating lagged features: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Date-Aware Datasets\n",
    "\n",
    "Now we'll create the date-aware datasets that will be used to feed data into the model. This step organizes the lagged features into the specific tensor structures expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets directly using IndexMappingDataset\n",
    "try:\n",
    "    lag_list = list(range(1, settings.LOOK_BACK_WINDOW + 1))\n",
    "    \n",
    "    train_dataset_indexed = IndexMappingDataset(\n",
    "        train_dataset,\n",
    "        lag_list,\n",
    "        h\n",
    "    )\n",
    "    \n",
    "    test_dataset_indexed = IndexMappingDataset(\n",
    "        test_dataset,\n",
    "        lag_list,\n",
    "        h\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    batch_size = settings.BATCH_SIZE\n",
    "    dataloader_train = DataLoader(\n",
    "        train_dataset_indexed,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    dataloader_test = DataLoader(\n",
    "        test_dataset_indexed,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset length: {len(train_dataset_indexed)}\")\n",
    "    print(f\"Test dataset length: {len(test_dataset_indexed)}\")\n",
    "    print(f\"Number of batches in train dataloader: {len(dataloader_train)}\")\n",
    "    print(f\"Number of batches in test dataloader: {len(dataloader_test)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataloaders: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Model Inputs\n",
    "\n",
    "Now let's examine the actual tensors that are fed into the model. This will give us a clear understanding of the input shapes and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch from the test dataloader\n",
    "for batch in dataloader_test:\n",
    "    x_lag1, x_lag5, x_lag22, y = batch\n",
    "    break  # Just get the first batch\n",
    "\n",
    "# Print shapes\n",
    "print(\"\\n===== MODEL INPUT SHAPES =====\\n\")\n",
    "print(f\"x_lag1 shape: {x_lag1.shape}\")\n",
    "print(f\"x_lag5 shape: {x_lag5.shape}\")\n",
    "print(f\"x_lag22 shape: {x_lag22.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Explain the dimensions\n",
    "print(\"\\n===== DIMENSION EXPLANATION =====\\n\")\n",
    "print(f\"x_lag1: [batch_size={x_lag1.shape[0]}, time_steps={x_lag1.shape[1]}, features={x_lag1.shape[2]}]\")\n",
    "print(f\"x_lag5: [batch_size={x_lag5.shape[0]}, time_steps={x_lag5.shape[1]}, features={x_lag5.shape[2]}]\")\n",
    "print(f\"x_lag22: [batch_size={x_lag22.shape[0]}, time_steps={x_lag22.shape[1]}, features={x_lag22.shape[2]}]\")\n",
    "print(f\"y: [batch_size={y.shape[0]}, prediction_horizon={y.shape[1]}, features={y.shape[2]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely display tensor data\n",
    "def display_tensor_sample(tensor, name, sample_idx=0, max_rows=5, max_cols=10):\n",
    "    print(f\"\\n===== {name} TENSOR SAMPLE =====\\n\")\n",
    "    print(f\"Shape: {tensor.shape}\")\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    sample = tensor[sample_idx].numpy()\n",
    "    \n",
    "    # Create a DataFrame with appropriate columns\n",
    "    if sample.ndim == 1:\n",
    "        # 1D tensor - just show as a single row\n",
    "        print(f\"First {min(max_cols, len(sample))} values: {sample[:max_cols]}\")\n",
    "        if len(sample) > max_cols:\n",
    "            print(f\"Last {min(max_cols, len(sample))} values: {sample[-max_cols:]}\")\n",
    "    elif sample.ndim == 2:\n",
    "        # 2D tensor - show as a table with row and column indices\n",
    "        rows, cols = sample.shape\n",
    "        \n",
    "        # Show a subset of the data\n",
    "        display_rows = min(rows, max_rows)\n",
    "        display_cols = min(cols, max_cols)\n",
    "        \n",
    "        print(f\"Showing first {display_rows} rows and first {display_cols} columns:\")\n",
    "        subset = sample[:display_rows, :display_cols]\n",
    "        \n",
    "        # Create a DataFrame for better display\n",
    "        df = pd.DataFrame(subset)\n",
    "        display(df)\n",
    "        \n",
    "        if cols > max_cols:\n",
    "            print(f\"... and {cols - max_cols} more columns\")\n",
    "        if rows > max_rows:\n",
    "            print(f\"... and {rows - max_rows} more rows\")\n",
    "    else:\n",
    "        print(f\"Cannot display tensor with {sample.ndim} dimensions\")\n",
    "\n",
    "# Display tensor samples\n",
    "display_tensor_sample(x_lag1, \"x_lag1 (MOST RECENT LAG)\")\n",
    "display_tensor_sample(x_lag5, \"x_lag5 (5-DAY LAGS)\")\n",
    "display_tensor_sample(x_lag22, \"x_lag22 (22-DAY LAGS)\")\n",
    "display_tensor_sample(y, \"y (TARGET VALUES)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics for each input tensor\n",
    "print(\"\\n===== INPUT TENSOR STATISTICS =====\\n\")\n",
    "print(f\"x_lag1 mean: {x_lag1.mean().item():.4f}, std: {x_lag1.std().item():.4f}, min: {x_lag1.min().item():.4f}, max: {x_lag1.max().item():.4f}\")\n",
    "print(f\"x_lag5 mean: {x_lag5.mean().item():.4f}, std: {x_lag5.std().item():.4f}, min: {x_lag5.min().item():.4f}, max: {x_lag5.max().item():.4f}\")\n",
    "print(f\"x_lag22 mean: {x_lag22.mean().item():.4f}, std: {x_lag22.std().item():.4f}, min: {x_lag22.min().item():.4f}, max: {x_lag22.max().item():.4f}\")\n",
    "print(f\"y mean: {y.mean().item():.4f}, std: {y.std().item():.4f}, min: {y.min().item():.4f}, max: {y.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Input Data\n",
    "\n",
    "Let's visualize the input data to better understand the structure and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a single feature across all lags\n",
    "feature_idx = 0  # Just use the first feature\n",
    "sample_idx = 0   # First sample in the batch\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Extract values for the selected feature\n",
    "lag1_values = x_lag1[sample_idx, :, feature_idx].numpy().flatten()\n",
    "lag5_values = x_lag5[sample_idx, :, feature_idx].numpy().flatten()\n",
    "lag22_values = x_lag22[sample_idx, :, feature_idx].numpy().flatten()\n",
    "target_values = y[sample_idx, :, feature_idx].numpy().flatten()\n",
    "\n",
    "# Create x-axis values (days relative to prediction)\n",
    "lag1_x = [-1]  # Most recent lag\n",
    "lag5_x = list(range(-5, 0))  # 5-day lags\n",
    "lag22_x = list(range(-22, 0))  # 22-day lags\n",
    "target_x = list(range(len(target_values)))  # Future values\n",
    "\n",
    "# Plot\n",
    "plt.plot(lag22_x, lag22_values, 'bo-', label='22-day lags', alpha=0.7)\n",
    "plt.plot(lag5_x, lag5_values, 'go-', label='5-day lags', linewidth=2)\n",
    "plt.plot(lag1_x, lag1_values, 'ro-', label='1-day lag', markersize=8)\n",
    "plt.plot(target_x, target_values, 'mo-', label='target values', linewidth=2)\n",
    "\n",
    "plt.axvline(x=0, color='k', linestyle='--', label='prediction point')\n",
    "plt.title(f'Input and Target Values for Feature {feature_idx}', fontsize=16)\n",
    "plt.xlabel('Time (days relative to prediction point)', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple features for the most recent lag\n",
    "sample_idx = 0  # First sample in the batch\n",
    "num_features = min(10, x_lag1.shape[2])  # Show up to 10 features\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Extract values for multiple features\n",
    "for i in range(num_features):\n",
    "    feature_values = x_lag1[sample_idx, 0, i].item()\n",
    "    plt.bar(i, feature_values, alpha=0.7)\n",
    "\n",
    "plt.title('Most Recent Lag Values for Multiple Features', fontsize=16)\n",
    "plt.xlabel('Feature Index', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation between features\n",
    "sample_idx = 0  # First sample in the batch\n",
    "time_idx = 0    # Most recent time step\n",
    "num_features = min(20, x_lag1.shape[2])  # Show up to 20 features\n",
    "\n",
    "# Extract feature values from all three lag tensors\n",
    "lag1_features = x_lag1[sample_idx, 0, :num_features].numpy()\n",
    "lag5_features = x_lag5[sample_idx, 0, :num_features].numpy()  # Most recent of the 5-day lags\n",
    "lag22_features = x_lag22[sample_idx, 0, :num_features].numpy()  # Most recent of the 22-day lags\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "feature_df = pd.DataFrame({\n",
    "    'lag1': lag1_features,\n",
    "    'lag5': lag5_features,\n",
    "    'lag22': lag22_features\n",
    "})\n",
    "\n",
    "# Calculate and visualize the correlation matrix\n",
    "corr_matrix = feature_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Correlation Between Different Lag Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of Model Input Structure\n",
    "\n",
    "Based on our inspection, here's a summary of the GSPHAR model's input structure:\n",
    "\n",
    "1. **Input Tensors**:\n",
    "   - `x_lag1`: Shape [batch_size, 1, num_features] - Most recent lag (t-1)\n",
    "   - `x_lag5`: Shape [batch_size, 5, num_features] - 5-day lags (t-5 to t-1)\n",
    "   - `x_lag22`: Shape [batch_size, 22, num_features] - 22-day lags (t-22 to t-1)\n",
    "\n",
    "2. **Target Tensor**:\n",
    "   - `y`: Shape [batch_size, horizon, num_features] - Future values (t to t+horizon-1)\n",
    "\n",
    "3. **Data Transformation Process**:\n",
    "   - Raw time series data → Split into train/test → Create lagged features → Create date-aware datasets → Generate batches\n",
    "\n",
    "4. **Key Characteristics**:\n",
    "   - Multi-scale approach captures both short-term and long-term patterns\n",
    "   - Date-aware datasets maintain the connection between tensor indices and actual dates\n",
    "   - Features show varying degrees of correlation across different time scales\n",
    "\n",
    "This structure allows the GSPHAR model to effectively capture temporal dependencies and cross-cryptocurrency relationships for volatility forecasting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
