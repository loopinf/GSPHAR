{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of GSPHAR and GARCH Models\n",
    "\n",
    "This notebook compares the performance of the trained GSPHAR model with a traditional GARCH model for volatility forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Add the parent directory to the path to import from the GSPHAR package\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import from local modules\n",
    "from config import settings\n",
    "from src.data import load_data, split_data, create_lagged_features, prepare_data_dict, create_dataloaders\n",
    "from src.models import GSPHAR\n",
    "from src.utils.graph_utils import compute_spillover_index\n",
    "from src.utils.model_utils import load_model\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (3421, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.FCHI</th>\n",
       "      <th>.AEX</th>\n",
       "      <th>.BFX</th>\n",
       "      <th>.STOXX50E</th>\n",
       "      <th>.IBEX</th>\n",
       "      <th>.GDAXI</th>\n",
       "      <th>.AORD</th>\n",
       "      <th>.FTSE</th>\n",
       "      <th>.MXX</th>\n",
       "      <th>.IXIC</th>\n",
       "      <th>...</th>\n",
       "      <th>.BSESN</th>\n",
       "      <th>.NSEI</th>\n",
       "      <th>.KS11</th>\n",
       "      <th>.BVSP</th>\n",
       "      <th>.HSI</th>\n",
       "      <th>.KSE</th>\n",
       "      <th>.N225</th>\n",
       "      <th>.SSEC</th>\n",
       "      <th>.OSEAX</th>\n",
       "      <th>.GSPTSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-05-08</th>\n",
       "      <td>0.800809</td>\n",
       "      <td>0.651051</td>\n",
       "      <td>0.504604</td>\n",
       "      <td>1.130383</td>\n",
       "      <td>0.801383</td>\n",
       "      <td>1.335785</td>\n",
       "      <td>0.491402</td>\n",
       "      <td>0.994279</td>\n",
       "      <td>0.891513</td>\n",
       "      <td>1.557359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.562364</td>\n",
       "      <td>0.316141</td>\n",
       "      <td>1.137436</td>\n",
       "      <td>0.969893</td>\n",
       "      <td>0.976046</td>\n",
       "      <td>1.713098</td>\n",
       "      <td>1.608609</td>\n",
       "      <td>0.598224</td>\n",
       "      <td>0.687013</td>\n",
       "      <td>0.964418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-05-10</th>\n",
       "      <td>0.941475</td>\n",
       "      <td>0.990710</td>\n",
       "      <td>0.675708</td>\n",
       "      <td>1.097318</td>\n",
       "      <td>1.193052</td>\n",
       "      <td>1.349511</td>\n",
       "      <td>0.394249</td>\n",
       "      <td>0.700422</td>\n",
       "      <td>0.537736</td>\n",
       "      <td>1.691661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613983</td>\n",
       "      <td>0.221990</td>\n",
       "      <td>1.726133</td>\n",
       "      <td>1.489246</td>\n",
       "      <td>0.932407</td>\n",
       "      <td>1.137886</td>\n",
       "      <td>0.636244</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>0.789587</td>\n",
       "      <td>0.353633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-05-13</th>\n",
       "      <td>1.055522</td>\n",
       "      <td>0.779671</td>\n",
       "      <td>0.558409</td>\n",
       "      <td>1.059856</td>\n",
       "      <td>1.299618</td>\n",
       "      <td>1.062080</td>\n",
       "      <td>0.401988</td>\n",
       "      <td>0.542140</td>\n",
       "      <td>0.534188</td>\n",
       "      <td>1.437287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449468</td>\n",
       "      <td>0.376285</td>\n",
       "      <td>1.375666</td>\n",
       "      <td>0.863537</td>\n",
       "      <td>0.955002</td>\n",
       "      <td>1.093345</td>\n",
       "      <td>0.638540</td>\n",
       "      <td>0.536356</td>\n",
       "      <td>0.619344</td>\n",
       "      <td>0.481264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-05-14</th>\n",
       "      <td>0.996049</td>\n",
       "      <td>0.982835</td>\n",
       "      <td>0.753802</td>\n",
       "      <td>1.053720</td>\n",
       "      <td>1.114927</td>\n",
       "      <td>1.189300</td>\n",
       "      <td>0.356502</td>\n",
       "      <td>0.753833</td>\n",
       "      <td>0.569891</td>\n",
       "      <td>1.184385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.640427</td>\n",
       "      <td>0.507480</td>\n",
       "      <td>1.390511</td>\n",
       "      <td>1.317750</td>\n",
       "      <td>0.816236</td>\n",
       "      <td>0.786833</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.641798</td>\n",
       "      <td>0.769300</td>\n",
       "      <td>0.434225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-05-15</th>\n",
       "      <td>1.021584</td>\n",
       "      <td>1.004905</td>\n",
       "      <td>0.762836</td>\n",
       "      <td>1.223201</td>\n",
       "      <td>1.210052</td>\n",
       "      <td>1.310291</td>\n",
       "      <td>0.828365</td>\n",
       "      <td>0.657404</td>\n",
       "      <td>0.563402</td>\n",
       "      <td>1.724872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859792</td>\n",
       "      <td>0.343130</td>\n",
       "      <td>0.948962</td>\n",
       "      <td>1.079232</td>\n",
       "      <td>0.741368</td>\n",
       "      <td>0.623714</td>\n",
       "      <td>1.306815</td>\n",
       "      <td>0.736604</td>\n",
       "      <td>0.646470</td>\n",
       "      <td>0.626194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               .FCHI      .AEX      .BFX  .STOXX50E     .IBEX    .GDAXI  \\\n",
       "2002-05-08  0.800809  0.651051  0.504604   1.130383  0.801383  1.335785   \n",
       "2002-05-10  0.941475  0.990710  0.675708   1.097318  1.193052  1.349511   \n",
       "2002-05-13  1.055522  0.779671  0.558409   1.059856  1.299618  1.062080   \n",
       "2002-05-14  0.996049  0.982835  0.753802   1.053720  1.114927  1.189300   \n",
       "2002-05-15  1.021584  1.004905  0.762836   1.223201  1.210052  1.310291   \n",
       "\n",
       "               .AORD     .FTSE      .MXX     .IXIC  ...    .BSESN     .NSEI  \\\n",
       "2002-05-08  0.491402  0.994279  0.891513  1.557359  ...  0.562364  0.316141   \n",
       "2002-05-10  0.394249  0.700422  0.537736  1.691661  ...  0.613983  0.221990   \n",
       "2002-05-13  0.401988  0.542140  0.534188  1.437287  ...  0.449468  0.376285   \n",
       "2002-05-14  0.356502  0.753833  0.569891  1.184385  ...  0.640427  0.507480   \n",
       "2002-05-15  0.828365  0.657404  0.563402  1.724872  ...  0.859792  0.343130   \n",
       "\n",
       "               .KS11     .BVSP      .HSI      .KSE     .N225     .SSEC  \\\n",
       "2002-05-08  1.137436  0.969893  0.976046  1.713098  1.608609  0.598224   \n",
       "2002-05-10  1.726133  1.489246  0.932407  1.137886  0.636244  0.498349   \n",
       "2002-05-13  1.375666  0.863537  0.955002  1.093345  0.638540  0.536356   \n",
       "2002-05-14  1.390511  1.317750  0.816236  0.786833  0.780142  0.641798   \n",
       "2002-05-15  0.948962  1.079232  0.741368  0.623714  1.306815  0.736604   \n",
       "\n",
       "              .OSEAX   .GSPTSE  \n",
       "2002-05-08  0.687013  0.964418  \n",
       "2002-05-10  0.789587  0.353633  \n",
       "2002-05-13  0.619344  0.481264  \n",
       "2002-05-14  0.769300  0.434225  \n",
       "2002-05-15  0.646470  0.626194  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data_file = '../data/rv5_sqrt_24.csv'  # Use relative path from notebooks directory\n",
    "data = load_data(data_file)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (2394, 24)\n",
      "Test data shape: (1027, 24)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "train_split = 0.7\n",
    "train_dataset_raw, test_dataset_raw = split_data(data, train_split)\n",
    "print(f\"Train data shape: {train_dataset_raw.shape}\")\n",
    "print(f\"Test data shape: {test_dataset_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained GSPHAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing data dictionary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2368/2368 [00:04<00:00, 478.87it/s]\n",
      "Preparing data dictionary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1001/1001 [00:02<00:00, 488.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "h = settings.PREDICTION_HORIZON\n",
    "look_back = settings.LOOK_BACK_WINDOW\n",
    "input_dim = settings.INPUT_DIM\n",
    "output_dim = settings.OUTPUT_DIM\n",
    "filter_size = settings.FILTER_SIZE\n",
    "\n",
    "# Get market indices\n",
    "market_indices_list = train_dataset_raw.columns.tolist()\n",
    "\n",
    "# Compute spillover index\n",
    "DY_adj = compute_spillover_index(train_dataset_raw, h, look_back, 0.0, standardized=True)\n",
    "\n",
    "# Create lagged features\n",
    "train_dataset = create_lagged_features(train_dataset_raw, market_indices_list, h, look_back)\n",
    "test_dataset = create_lagged_features(test_dataset_raw, market_indices_list, h, look_back)\n",
    "\n",
    "# Prepare data dictionaries\n",
    "train_dict = prepare_data_dict(train_dataset, market_indices_list, look_back)\n",
    "test_dict = prepare_data_dict(test_dataset, market_indices_list, look_back)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = settings.BATCH_SIZE\n",
    "dataloader_train, dataloader_test = create_dataloaders(train_dict, test_dict, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/GSPHAR_24_magnet_dynamic_h5.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load trained model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_save_name \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mMODEL_SAVE_NAME_PATTERN\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      6\u001b[0m     filter_size\u001b[38;5;241m=\u001b[39mfilter_size,\n\u001b[1;32m      7\u001b[0m     h\u001b[38;5;241m=\u001b[39mh\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m trained_model, mae_loss \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/GSPHAR/src/utils/model_utils.py:46\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, model)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(name, model):\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    Load a model.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m        tuple: (model, mae_loss)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.tar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     48\u001b[0m     num_L \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/cenv_njs_main/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniforge3/envs/cenv_njs_main/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniforge3/envs/cenv_njs_main/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/GSPHAR_24_magnet_dynamic_h5.tar'"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = GSPHAR(input_dim, output_dim, filter_size, DY_adj)\n",
    "\n",
    "# Load trained model\n",
    "model_save_name = settings.MODEL_SAVE_NAME_PATTERN.format(\n",
    "    filter_size=filter_size,\n",
    "    h=h\n",
    ")\n",
    "# Make sure we're using the correct path\n",
    "model_path = os.path.join('..', 'models', f'{model_save_name}.tar')\n",
    "if os.path.exists(model_path):\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    mae_loss = checkpoint['loss']\n",
    "    print(f\"Loaded model: {model_save_name}\")\n",
    "    print(f\"MAE loss: {mae_loss:.4f}\")\n",
    "    trained_model = model\n",
    "else:\n",
    "    print(f\"Model file not found: {model_path}\")\n",
    "    print(\"Available models:\")\n",
    "    for model_file in os.listdir(os.path.join('..', 'models')):\n",
    "        print(f\"  - {model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate GSPHAR Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gsphar_predictions(model, dataloader, market_indices_list, test_dates=None):\n",
    "    \"\"\"Generate predictions using the GSPHAR model.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_lag1, x_lag5, x_lag22, y = batch\n",
    "            \n",
    "            # Move to CPU for consistent comparison\n",
    "            x_lag1 = x_lag1.cpu()\n",
    "            x_lag5 = x_lag5.cpu()\n",
    "            x_lag22 = x_lag22.cpu()\n",
    "            \n",
    "            # Generate predictions\n",
    "            output, _, _ = model(x_lag1, x_lag5, x_lag22)\n",
    "            \n",
    "            # Store predictions and actuals\n",
    "            all_predictions.append(output.cpu().numpy())\n",
    "            all_actuals.append(y.numpy())\n",
    "    \n",
    "    # Concatenate all predictions and actuals\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_actuals = np.vstack(all_actuals)\n",
    "    \n",
    "    # Create DataFrames with proper datetime index if provided\n",
    "    if test_dates is not None:\n",
    "        # Make sure we have the right number of dates\n",
    "        if len(test_dates) >= len(all_predictions):\n",
    "            # Use the last portion of test_dates that matches our prediction length\n",
    "            dates_to_use = test_dates[-len(all_predictions):]\n",
    "            pred_df = pd.DataFrame(all_predictions, index=dates_to_use, columns=market_indices_list)\n",
    "            actual_df = pd.DataFrame(all_actuals, index=dates_to_use, columns=market_indices_list)\n",
    "        else:\n",
    "            print(\"Warning: Not enough dates provided for predictions. Using default index.\")\n",
    "            pred_df = pd.DataFrame(all_predictions, columns=market_indices_list)\n",
    "            actual_df = pd.DataFrame(all_actuals, columns=market_indices_list)\n",
    "    else:\n",
    "        pred_df = pd.DataFrame(all_predictions, columns=market_indices_list)\n",
    "        actual_df = pd.DataFrame(all_actuals, columns=market_indices_list)\n",
    "    \n",
    "    return pred_df, actual_df\n",
    "\n",
    "# Get the test dates from the test dataset\n",
    "test_dates = test_dataset_raw.index\n",
    "\n",
    "# Generate predictions with dates\n",
    "gsphar_pred_df, gsphar_actual_df = generate_gsphar_predictions(trained_model, dataloader_test, market_indices_list, test_dates)\n",
    "\n",
    "print(\"GSPHAR predictions shape:\", gsphar_pred_df.shape)\n",
    "print(\"GSPHAR actuals shape:\", gsphar_actual_df.shape)\n",
    "print(\"GSPHAR predictions index type:\", type(gsphar_pred_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_garch_model(series, p=1, q=1):\n",
    "    \"\"\"Fit a GARCH(p,q) model to the given series.\"\"\"\n",
    "    model = arch_model(series, vol='Garch', p=p, q=q, rescale=False)\n",
    "    result = model.fit(disp='off')\n",
    "    return result\n",
    "\n",
    "def generate_garch_predictions(train_data, test_data, market_indices_list, p=1, q=1, horizon=5):\n",
    "    \"\"\"Generate predictions using GARCH models for each market index.\"\"\"\n",
    "    predictions = {}\n",
    "    actuals = {}\n",
    "    models = {}\n",
    "    \n",
    "    # Get the test dates\n",
    "    test_dates = test_data.index\n",
    "    \n",
    "    # For each market index\n",
    "    for market_index in tqdm(market_indices_list, desc=\"Fitting GARCH models\"):\n",
    "        # Get the training data for this market index\n",
    "        train_series = train_data[market_index]\n",
    "        \n",
    "        # Fit GARCH model\n",
    "        model = fit_garch_model(train_series, p=p, q=q)\n",
    "        models[market_index] = model\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = model.forecast(horizon=horizon, reindex=False)\n",
    "        \n",
    "        # Extract the h-step ahead variance forecast and take square root for volatility\n",
    "        variance_forecast = forecasts.variance.iloc[-1, horizon-1]\n",
    "        volatility_forecast = np.sqrt(variance_forecast)\n",
    "        \n",
    "        # Store prediction and actual\n",
    "        predictions[market_index] = [volatility_forecast]\n",
    "        actuals[market_index] = [test_data[market_index].iloc[0]]\n",
    "        \n",
    "        # For the rest of the test data, use rolling forecasts\n",
    "        for i in range(1, len(test_data)):\n",
    "            # Update the model with the latest observation\n",
    "            updated_data = pd.concat([train_series, test_data[market_index].iloc[:i]])\n",
    "            model = fit_garch_model(updated_data, p=p, q=q)\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecasts = model.forecast(horizon=horizon, reindex=False)\n",
    "            variance_forecast = forecasts.variance.iloc[-1, horizon-1]\n",
    "            volatility_forecast = np.sqrt(variance_forecast)\n",
    "            \n",
    "            # Store prediction and actual\n",
    "            predictions[market_index].append(volatility_forecast)\n",
    "            if i < len(test_data):\n",
    "                actuals[market_index].append(test_data[market_index].iloc[i])\n",
    "    \n",
    "    # Convert to DataFrames with datetime index\n",
    "    pred_df = pd.DataFrame(predictions, index=test_dates[:len(predictions[market_indices_list[0]])])\n",
    "    actual_df = pd.DataFrame(actuals, index=test_dates[:len(actuals[market_indices_list[0]])])\n",
    "    \n",
    "    return pred_df, actual_df, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GARCH predictions for a subset of indices to save time\n",
    "# You can increase this number or use all indices if you have time\n",
    "subset_indices = market_indices_list[:3]  # Use first 3 indices\n",
    "print(f\"Using subset of indices for GARCH: {subset_indices}\")\n",
    "\n",
    "garch_pred_df, garch_actual_df, garch_models = generate_garch_predictions(\n",
    "    train_dataset_raw[subset_indices], \n",
    "    test_dataset_raw[subset_indices], \n",
    "    subset_indices,\n",
    "    p=1, q=1, horizon=h\n",
    ")\n",
    "\n",
    "print(\"GARCH predictions shape:\", garch_pred_df.shape)\n",
    "print(\"GARCH actuals shape:\", garch_actual_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, actuals, market_indices):\n",
    "    \"\"\"Calculate performance metrics for each market index.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for market_index in market_indices:\n",
    "        y_pred = predictions[market_index]\n",
    "        y_true = actuals[market_index]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        metrics[market_index] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae\n",
    "        }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for GSPHAR model\n",
    "gsphar_metrics = calculate_metrics(\n",
    "    gsphar_pred_df[subset_indices], \n",
    "    gsphar_actual_df[subset_indices], \n",
    "    subset_indices\n",
    ")\n",
    "\n",
    "# Calculate metrics for GARCH model\n",
    "garch_metrics = calculate_metrics(\n",
    "    garch_pred_df, \n",
    "    garch_actual_df, \n",
    "    subset_indices\n",
    ")\n",
    "\n",
    "# Display metrics\n",
    "print(\"GSPHAR Model Metrics:\")\n",
    "for market_index, metrics in gsphar_metrics.items():\n",
    "    print(f\"\\n{market_index}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"GARCH Model Metrics:\")\n",
    "for market_index, metrics in garch_metrics.items():\n",
    "    print(f\"\\n{market_index}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(gsphar_pred, gsphar_actual, garch_pred, garch_actual, market_index):\n",
    "    \"\"\"Plot predictions from both models for a given market index using Plotly.\"\"\"\n",
    "    import pandas as pd\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    # Set Plotly as the backend for pandas plotting\n",
    "    pd.options.plotting.backend = \"plotly\"\n",
    "    \n",
    "    # Get the dates from both actual datasets\n",
    "    gsphar_dates = gsphar_actual.index\n",
    "    garch_dates = garch_actual.index\n",
    "    \n",
    "    # Find common date range\n",
    "    common_dates = gsphar_dates.intersection(garch_dates)\n",
    "    \n",
    "    if len(common_dates) == 0:\n",
    "        # If no common dates, use the test dataset dates\n",
    "        print(\"Warning: No common dates found between GSPHAR and GARCH predictions.\")\n",
    "        print(\"Using GSPHAR dates for plotting.\")\n",
    "        \n",
    "        # Create separate figures for each model\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add GSPHAR data\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=gsphar_dates,\n",
    "            y=gsphar_actual[market_index],\n",
    "            mode='lines',\n",
    "            name='Actual'\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=gsphar_dates,\n",
    "            y=gsphar_pred[market_index],\n",
    "            mode='lines',\n",
    "            name='GSPHAR'\n",
    "        ))\n",
    "        \n",
    "        # Add GARCH data with secondary x-axis\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=garch_dates,\n",
    "            y=garch_pred[market_index],\n",
    "            mode='lines',\n",
    "            name='GARCH'\n",
    "        ))\n",
    "    else:\n",
    "        # Filter data to common date range\n",
    "        gsphar_actual_common = gsphar_actual.loc[common_dates, market_index]\n",
    "        gsphar_pred_common = gsphar_pred.loc[common_dates, market_index]\n",
    "        garch_pred_common = garch_pred.loc[common_dates, market_index]\n",
    "        \n",
    "        # Create DataFrame with aligned data\n",
    "        df_plot = pd.DataFrame({\n",
    "            'Actual': gsphar_actual_common,\n",
    "            'GSPHAR': gsphar_pred_common,\n",
    "            'GARCH': garch_pred_common\n",
    "        }, index=common_dates)\n",
    "        \n",
    "        # Create the plot\n",
    "        fig = df_plot.plot(\n",
    "            title=f'Volatility Predictions for {market_index}',\n",
    "            labels=dict(index=\"Date\", value=\"Volatility\"),\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "    \n",
    "    # Update layout for better appearance\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "        hovermode=\"x unified\",\n",
    "        title={\n",
    "            'text': f'Volatility Predictions for {market_index}',\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add grid\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey',\n",
    "                    tickangle=45, tickformat='%Y-%m-%d')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for each market index in the subset\n",
    "for market_index in subset_indices:\n",
    "    plot_predictions(\n",
    "        gsphar_pred_df, \n",
    "        gsphar_actual_df, \n",
    "        garch_pred_df, \n",
    "        garch_actual_df, \n",
    "        market_index\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we compared the performance of the GSPHAR model with a traditional GARCH model for volatility forecasting. The comparison was based on several metrics including MSE, RMSE, and MAE.\n",
    "\n",
    "Key findings:\n",
    "1. [Add your observations about which model performed better]\n",
    "2. [Add insights about the strengths and weaknesses of each model]\n",
    "3. [Add any other relevant conclusions]\n",
    "\n",
    "Future work could include:\n",
    "1. Testing with different GARCH specifications (e.g., EGARCH, GJR-GARCH)\n",
    "2. Extending the comparison to more market indices\n",
    "3. Implementing a hybrid model that combines the strengths of both approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
