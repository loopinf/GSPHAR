{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of GSPHAR and GARCH Models\n",
    "\n",
    "This notebook compares the performance of the trained GSPHAR model with a traditional GARCH model for volatility forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Add the parent directory to the path to import from the GSPHAR package\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import from local modules\n",
    "from config import settings\n",
    "from src.data import load_data, split_data, create_lagged_features, prepare_data_dict, create_dataloaders\n",
    "from src.models import GSPHAR\n",
    "from src.utils.graph_utils import compute_spillover_index\n",
    "from src.utils.model_utils import load_model\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_file = '../data/rv5_sqrt_24.csv'  # Use relative path from notebooks directory\n",
    "data = load_data(data_file)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_split = 0.7\n",
    "train_dataset_raw, test_dataset_raw = split_data(data, train_split)\n",
    "print(f\"Train data shape: {train_dataset_raw.shape}\")\n",
    "print(f\"Test data shape: {test_dataset_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained GSPHAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "h = settings.PREDICTION_HORIZON\n",
    "look_back = settings.LOOK_BACK_WINDOW\n",
    "input_dim = settings.INPUT_DIM\n",
    "output_dim = settings.OUTPUT_DIM\n",
    "filter_size = settings.FILTER_SIZE\n",
    "\n",
    "# Get market indices\n",
    "market_indices_list = train_dataset_raw.columns.tolist()\n",
    "\n",
    "# Compute spillover index\n",
    "DY_adj = compute_spillover_index(train_dataset_raw, h, look_back, 0.0, standardized=True)\n",
    "\n",
    "# Create lagged features\n",
    "train_dataset = create_lagged_features(train_dataset_raw, market_indices_list, h, look_back)\n",
    "test_dataset = create_lagged_features(test_dataset_raw, market_indices_list, h, look_back)\n",
    "\n",
    "# Prepare data dictionaries\n",
    "train_dict = prepare_data_dict(train_dataset, market_indices_list, look_back)\n",
    "test_dict = prepare_data_dict(test_dataset, market_indices_list, look_back)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = settings.BATCH_SIZE\n",
    "dataloader_train, dataloader_test = create_dataloaders(train_dict, test_dict, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = GSPHAR(input_dim, output_dim, filter_size, DY_adj)\n",
    "\n",
    "# Load trained model\n",
    "model_save_name = settings.MODEL_SAVE_NAME_PATTERN.format(\n",
    "    filter_size=filter_size,\n",
    "    h=h\n",
    ")\n",
    "# Make sure we're using the correct path\n",
    "model_path = os.path.join('..', 'models', f'{model_save_name}.tar')\n",
    "if os.path.exists(model_path):\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    mae_loss = checkpoint['loss']\n",
    "    print(f\"Loaded model: {model_save_name}\")\n",
    "    print(f\"MAE loss: {mae_loss:.4f}\")\n",
    "    trained_model = model\n",
    "else:\n",
    "    print(f\"Model file not found: {model_path}\")\n",
    "    print(\"Available models:\")\n",
    "    for model_file in os.listdir(os.path.join('..', 'models')):\n",
    "        print(f\"  - {model_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate GSPHAR Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gsphar_predictions(model, dataloader, market_indices_list, test_dates=None):\n",
    "    \"\"\"Generate predictions using the GSPHAR model.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_lag1, x_lag5, x_lag22, y = batch\n",
    "            \n",
    "            # Move to CPU for consistent comparison\n",
    "            x_lag1 = x_lag1.cpu()\n",
    "            x_lag5 = x_lag5.cpu()\n",
    "            x_lag22 = x_lag22.cpu()\n",
    "            \n",
    "            # Generate predictions\n",
    "            output, _, _ = model(x_lag1, x_lag5, x_lag22)\n",
    "            \n",
    "            # Store predictions and actuals\n",
    "            all_predictions.append(output.cpu().numpy())\n",
    "            all_actuals.append(y.numpy())\n",
    "    \n",
    "    # Concatenate all predictions and actuals\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_actuals = np.vstack(all_actuals)\n",
    "    \n",
    "    # Create DataFrames with proper datetime index if provided\n",
    "    if test_dates is not None:\n",
    "        # Make sure we have the right number of dates\n",
    "        if len(test_dates) >= len(all_predictions):\n",
    "            # Use the last portion of test_dates that matches our prediction length\n",
    "            dates_to_use = test_dates[-len(all_predictions):]\n",
    "            pred_df = pd.DataFrame(all_predictions, index=dates_to_use, columns=market_indices_list)\n",
    "            actual_df = pd.DataFrame(all_actuals, index=dates_to_use, columns=market_indices_list)\n",
    "        else:\n",
    "            print(\"Warning: Not enough dates provided for predictions. Using default index.\")\n",
    "            pred_df = pd.DataFrame(all_predictions, columns=market_indices_list)\n",
    "            actual_df = pd.DataFrame(all_actuals, columns=market_indices_list)\n",
    "    else:\n",
    "        pred_df = pd.DataFrame(all_predictions, columns=market_indices_list)\n",
    "        actual_df = pd.DataFrame(all_actuals, columns=market_indices_list)\n",
    "    \n",
    "    return pred_df, actual_df\n",
    "\n",
    "# Get the test dates from the test dataset\n",
    "test_dates = test_dataset_raw.index\n",
    "\n",
    "# Generate predictions with dates\n",
    "gsphar_pred_df, gsphar_actual_df = generate_gsphar_predictions(trained_model, dataloader_test, market_indices_list, test_dates)\n",
    "\n",
    "print(\"GSPHAR predictions shape:\", gsphar_pred_df.shape)\n",
    "print(\"GSPHAR actuals shape:\", gsphar_actual_df.shape)\n",
    "print(\"GSPHAR predictions index type:\", type(gsphar_pred_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_garch_model(series, p=1, q=1):\n",
    "    \"\"\"Fit a GARCH(p,q) model to the given series.\"\"\"\n",
    "    model = arch_model(series, vol='Garch', p=p, q=q, rescale=False)\n",
    "    result = model.fit(disp='off')\n",
    "    return result\n",
    "\n",
    "def generate_garch_predictions(train_data, test_data, market_indices_list, p=1, q=1, horizon=5):\n",
    "    \"\"\"Generate predictions using GARCH models for each market index.\"\"\"\n",
    "    predictions = {}\n",
    "    actuals = {}\n",
    "    models = {}\n",
    "    \n",
    "    # Get the test dates\n",
    "    test_dates = test_data.index\n",
    "    \n",
    "    # For each market index\n",
    "    for market_index in tqdm(market_indices_list, desc=\"Fitting GARCH models\"):\n",
    "        # Get the training data for this market index\n",
    "        train_series = train_data[market_index]\n",
    "        \n",
    "        # Fit GARCH model\n",
    "        model = fit_garch_model(train_series, p=p, q=q)\n",
    "        models[market_index] = model\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = model.forecast(horizon=horizon, reindex=False)\n",
    "        \n",
    "        # Extract the h-step ahead variance forecast and take square root for volatility\n",
    "        variance_forecast = forecasts.variance.iloc[-1, horizon-1]\n",
    "        volatility_forecast = np.sqrt(variance_forecast)\n",
    "        \n",
    "        # Store prediction and actual\n",
    "        predictions[market_index] = [volatility_forecast]\n",
    "        actuals[market_index] = [test_data[market_index].iloc[0]]\n",
    "        \n",
    "        # For the rest of the test data, use rolling forecasts\n",
    "        for i in range(1, len(test_data)):\n",
    "            # Update the model with the latest observation\n",
    "            updated_data = pd.concat([train_series, test_data[market_index].iloc[:i]])\n",
    "            model = fit_garch_model(updated_data, p=p, q=q)\n",
    "            \n",
    "            # Generate forecast\n",
    "            forecasts = model.forecast(horizon=horizon, reindex=False)\n",
    "            variance_forecast = forecasts.variance.iloc[-1, horizon-1]\n",
    "            volatility_forecast = np.sqrt(variance_forecast)\n",
    "            \n",
    "            # Store prediction and actual\n",
    "            predictions[market_index].append(volatility_forecast)\n",
    "            if i < len(test_data):\n",
    "                actuals[market_index].append(test_data[market_index].iloc[i])\n",
    "    \n",
    "    # Convert to DataFrames with datetime index\n",
    "    pred_df = pd.DataFrame(predictions, index=test_dates[:len(predictions[market_indices_list[0]])])\n",
    "    actual_df = pd.DataFrame(actuals, index=test_dates[:len(actuals[market_indices_list[0]])])\n",
    "    \n",
    "    return pred_df, actual_df, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GARCH predictions for a subset of indices to save time\n",
    "# You can increase this number or use all indices if you have time\n",
    "subset_indices = market_indices_list[:3]  # Use first 3 indices\n",
    "print(f\"Using subset of indices for GARCH: {subset_indices}\")\n",
    "\n",
    "garch_pred_df, garch_actual_df, garch_models = generate_garch_predictions(\n",
    "    train_dataset_raw[subset_indices], \n",
    "    test_dataset_raw[subset_indices], \n",
    "    subset_indices,\n",
    "    p=1, q=1, horizon=h\n",
    ")\n",
    "\n",
    "print(\"GARCH predictions shape:\", garch_pred_df.shape)\n",
    "print(\"GARCH actuals shape:\", garch_actual_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, actuals, market_indices):\n",
    "    \"\"\"Calculate performance metrics for each market index.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for market_index in market_indices:\n",
    "        y_pred = predictions[market_index]\n",
    "        y_true = actuals[market_index]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        metrics[market_index] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae\n",
    "        }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for GSPHAR model\n",
    "gsphar_metrics = calculate_metrics(\n",
    "    gsphar_pred_df[subset_indices], \n",
    "    gsphar_actual_df[subset_indices], \n",
    "    subset_indices\n",
    ")\n",
    "\n",
    "# Calculate metrics for GARCH model\n",
    "garch_metrics = calculate_metrics(\n",
    "    garch_pred_df, \n",
    "    garch_actual_df, \n",
    "    subset_indices\n",
    ")\n",
    "\n",
    "# Display metrics\n",
    "print(\"GSPHAR Model Metrics:\")\n",
    "for market_index, metrics in gsphar_metrics.items():\n",
    "    print(f\"\\n{market_index}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"GARCH Model Metrics:\")\n",
    "for market_index, metrics in garch_metrics.items():\n",
    "    print(f\"\\n{market_index}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(gsphar_pred, gsphar_actual, garch_pred, garch_actual, market_index):\n",
    "    \"\"\"Plot predictions from both models for a given market index using Plotly.\"\"\"\n",
    "    import pandas as pd\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    # Set Plotly as the backend for pandas plotting\n",
    "    pd.options.plotting.backend = \"plotly\"\n",
    "    \n",
    "    # Get the dates from both actual datasets\n",
    "    gsphar_dates = gsphar_actual.index\n",
    "    garch_dates = garch_actual.index\n",
    "    \n",
    "    # Find common date range\n",
    "    common_dates = gsphar_dates.intersection(garch_dates)\n",
    "    \n",
    "    if len(common_dates) == 0:\n",
    "        # If no common dates, use the test dataset dates\n",
    "        print(\"Warning: No common dates found between GSPHAR and GARCH predictions.\")\n",
    "        print(\"Using GSPHAR dates for plotting.\")\n",
    "        \n",
    "        # Create separate figures for each model\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add GSPHAR data\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=gsphar_dates,\n",
    "            y=gsphar_actual[market_index],\n",
    "            mode='lines',\n",
    "            name='Actual'\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=gsphar_dates,\n",
    "            y=gsphar_pred[market_index],\n",
    "            mode='lines',\n",
    "            name='GSPHAR'\n",
    "        ))\n",
    "        \n",
    "        # Add GARCH data with secondary x-axis\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=garch_dates,\n",
    "            y=garch_pred[market_index],\n",
    "            mode='lines',\n",
    "            name='GARCH'\n",
    "        ))\n",
    "    else:\n",
    "        # Filter data to common date range\n",
    "        gsphar_actual_common = gsphar_actual.loc[common_dates, market_index]\n",
    "        gsphar_pred_common = gsphar_pred.loc[common_dates, market_index]\n",
    "        garch_pred_common = garch_pred.loc[common_dates, market_index]\n",
    "        \n",
    "        # Create DataFrame with aligned data\n",
    "        df_plot = pd.DataFrame({\n",
    "            'Actual': gsphar_actual_common,\n",
    "            'GSPHAR': gsphar_pred_common,\n",
    "            'GARCH': garch_pred_common\n",
    "        }, index=common_dates)\n",
    "        \n",
    "        # Create the plot\n",
    "        fig = df_plot.plot(\n",
    "            title=f'Volatility Predictions for {market_index}',\n",
    "            labels=dict(index=\"Date\", value=\"Volatility\"),\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "    \n",
    "    # Update layout for better appearance\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "        hovermode=\"x unified\",\n",
    "        title={\n",
    "            'text': f'Volatility Predictions for {market_index}',\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add grid\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey',\n",
    "                    tickangle=45, tickformat='%Y-%m-%d')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for each market index in the subset\n",
    "for market_index in subset_indices:\n",
    "    plot_predictions(\n",
    "        gsphar_pred_df, \n",
    "        gsphar_actual_df, \n",
    "        garch_pred_df, \n",
    "        garch_actual_df, \n",
    "        market_index\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we compared the performance of the GSPHAR model with a traditional GARCH model for volatility forecasting. The comparison was based on several metrics including MSE, RMSE, and MAE.\n",
    "\n",
    "Key findings:\n",
    "1. [Add your observations about which model performed better]\n",
    "2. [Add insights about the strengths and weaknesses of each model]\n",
    "3. [Add any other relevant conclusions]\n",
    "\n",
    "Future work could include:\n",
    "1. Testing with different GARCH specifications (e.g., EGARCH, GJR-GARCH)\n",
    "2. Extending the comparison to more market indices\n",
    "3. Implementing a hybrid model that combines the strengths of both approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
